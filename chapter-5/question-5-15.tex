Let $X_{ji}$ and $X_{jk}$ be the $i$th and $k$th components, respectively, of $\textbf{X}_{j}$.
\begin{enumerate}[label= (\alph*)]
    \item Show that $\mu_{i} = E(X_{ji}) = p_{i}$ and $\sigma_{ii} = \text{Var}(X_{ji}) = p_{i}(l - p_{i})$, $i = 1, 2, \dots, p$.
    
    Using the definition of discrete expectation for a single random variable,
    \[
        E[X] = \sum_{\ell}x_{\ell}P(X_{\ell} = x_{\ell})
    \]
    Here, we have a random matrix and for each component we have
    \[
        E[X_{ji}]
        =
        \sum_{\ell=1}^{k}x_{j\ell}P(X_{j\ell} = x_{j\ell})
        =
    \]
    \[{\scriptstyle 
        =
        0 \cdot P(X_{j1} = x_{j1}) + \dots + 1 \cdot P(X_{ji} = x_{ji}) + 0 \cdot P(X_{j,i+1} = x_{j,i+1}) + \dots + 0 \cdot P(X_{jk} = x_{jk})
        =
    }
    \]
    \[
        =
        0 \cdot p_{1} + \dots + 0 \cdot p_{i-1} + 1 \cdot p_{i} + 0 \cdot p_{i+1} + \dots + 0 \cdot p_{k}
        =
    \]
    \[
        =
        0 + \dots + 0 + p_{i} + 0 + \dots + 0
        =
    \]
    \[
        =
        p_{i}
    \]
    Similarly, the second moment is,
    \[
        E[X_{ji}X_{ji}]
        =
        E[X_{ji}^{2}]
        =
        \sum_{\ell=1}^{k}x_{j\ell}P(X_{j\ell} = x_{j\ell})
        =
    \]
    \[{\scriptstyle 
        =
        0^{2} \cdot P(X_{j1} = x_{j1}) + \dots + 1^{2} \cdot P(X_{ji} = x_{ji}) + 0^{2} \cdot P(X_{j,i+1} = x_{j,i+1}) + \dots + 0^{2} \cdot P(X_{jk} = x_{jk})
        =
    }
    \]
    \[
        =
        0^{2} \cdot p_{1} + \dots + 0^{2} \cdot p_{i-1} + 1^{2} \cdot p_{i} + 0^{2} \cdot p_{i+1} + \dots + 0^{2} \cdot p_{k}
        =
    \]
    \[
        =
        0 + \dots + 0 + p_{i} + 0 + \dots + 0
        =
    \]
    \[
        =
        p_{i}
    \]

    Using the definition for the variance for a single component,
    \[
        \text{Var}[X_{ji}]
        =
        E[{(X_{ji}-E[X_{ji}])}^{2}]
        =
        E[{(X_{ji}-p_{i})}^{2}]
        =
    \]
    \[
        =
        E[X_{ji}X_{ji} - X_{ji}p_{i} - p_{i}X_{ji} + {p_{i}}^{2}]
        =
        E[X_{ji}X_{ji}] - E[X_{ji}]p_{i} - p_{i}E[X_{ji}] + {p_{i}}^{2}
        =
    \]
    \[
        =
        p_{i} - p_{i}^{2} - p_{i}^{2} + {p_{i}}^{2}
        =
        p_{i} - p_{i}^{2}
        =
        p_{i}(1 - p_{i})
    \]
    For the $q+1$ category, we use a few things. One, $X_{j,q+1} = 1 - \left( \sum_{\ell=1}^{q}{X_{j\ell}} \right)$. Two, $p_{q+1} = 1 - \left( \sum_{\ell=1}^{q}{p_{\ell}} \right)$. Three, result from part (b). Four, the definition of the variance for sums of random variables, where for $Y = X_{1} + \cdots + X_{q}$, we have
    \[
        \text{Var}[Y]
        =
        \text{Var}[X_{1} + \cdots + X_{q}]
        =
        \sum_{i=1}^{q}{\text{Var}[X_{i}]}
        +
        2\sum_{i < j}\text{Cov}[X_{i}, X_{j}]
    \]
    Contructing the variance is then,
    \begin{align*}
        \text{Var}[X_{j,q+1}] 
        &=
        \text{Var}\left[
            1 - \left( \sum_{\ell=1}^{q}{X_{j\ell}} \right)
        \right] \\
        &=
        \scriptstyle
        \text{Var}\left[ 1 \right]
        +
        \sum_{\ell=1}^{q}{\text{Var}\left[X_{j\ell}\right]}
        +
        2\sum_{\ell}^{q}{\text{Cov}\left[1, X_{j\ell}\right]}
        +
        2\sum_{\ell<m}{\text{Cov}\left[X_{j\ell}, X_{jm}\right]} \\
        &=
        0
        +
        \sum_{\ell=1}^{q}{\text{Var}\left[X_{j\ell}\right]}
        +
        0
        +
        2\sum_{\ell<m}{\text{Cov}\left[X_{j\ell}, X_{jm}\right]} \\
        &=
        \sum_{\ell=1}^{q}{\text{Var}\left[X_{j\ell}\right]}
        +
        2\sum_{\ell<m}{\text{Cov}\left[X_{j\ell}, X_{jm}\right]}  \\
        &=
        \sum_{\ell=1}^{q}{p_{\ell}(1-p_{\ell})}
        +
        2\sum_{\ell<m}{(-p_{\ell}p_{m})} \\
        &=
        \sum_{\ell=1}^{q}{p_{\ell}} - \sum_{\ell=1}^{q}{p_{\ell}^{2}}
        -
        2\sum_{\ell<m}{p_{\ell}p_{m}} \\
        &=
        \sum_{\ell=1}^{q}{p_{\ell}}
        -
        \left( \sum_{\ell=1}^{q}{p_{\ell}^{2}} + 2\sum_{\ell<m}{p_{\ell}p_{m}} \right) \\
        &=
        \sum_{\ell=1}^{q}{p_{\ell}}
        -
        {\left( \sum_{\ell=1}^{q}{p_{\ell}} \right)}^{2} \\
        &=
        \left(
            1
            -
            \left( \sum_{\ell=1}^{q}{p_{\ell}} \right)
        \right)
        \left( \sum_{\ell=1}^{q}{p_{\ell}} \right) \\
        &=
        p_{q+1}\left(1 - p_{q+1} \right)
    \end{align*}

    \item Show that $\sigma_{ik} = \text{Cov}(X_{ji}, X_{jk}) = - p_{i}p_{k}$, $i \ne k$. Why must this covariance necessarily be negative?
    
    First, the expected value of $X_{ji}X_{jk}$ is
    \[
        E[X_{ji}X_{jk}]
        =
        \sum_{\ell=1}^{k}\sum_{m=1}^{k}x_{j\ell}x_{jm}P(X_{j\ell} = x_{j\ell}, X_{jm} = x_{jm})
        =
        0
    \]
    Because $x_{j\ell}x_{jm} = 0$ for all combinations of $\ell$ and $m$. This is because it's impossible for more than one value in our vector to take on the value 1, so we only have the cases, $0 \times 1 = 0$, $1 \times 0 = 0$, and $0 \times 0 = 0$.

    The covariance for the single components $i$ and $k$,
    \[
        \text{Cov}[X_{ji}, X_{jk}]
        =
        E[(X_{ji} - E[X_{ji}])(X_{jk} - E[X_{jk}])]
        =
    \]
    \[
        =
        E[(X_{ji} - p_{i})(X_{jk} - p_{k})]
        =
        E[X_{ji}X_{jk} - X_{ji}p_{k} - p_{i}X_{jk} + p_{i}p_{k}]
        =
    \]
    \[
        =
        E[X_{ji}X_{jk}] - E[X_{ji}]p_{k} - p_{i}E[X_{jk}] + p_{i}p_{k}
        =
        0 - p_{i}p_{k} - p_{i}p_{k} + p_{i}p_{k}
        =
    \]
    \[
        =
        - p_{i}p_{k}
    \]

    The covariance is necessarily negative because we only have two values, 0 and 1. For two components $i$ and $k$, when $i$ is ``large'' it has a value of 1 and the other component must be 0, which is smaller than 1. This is always the case, since only one component can be 1, and all others have to be zero. This describes the situation where the covariance is negative, where large values for one variable correspond to smaller values in the other, and vice versa.
\end{enumerate}
